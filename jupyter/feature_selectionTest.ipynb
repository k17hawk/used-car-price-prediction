{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, col, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master('local[*]').appName('used-car-price').config('spark.executor.memory', '6g').getOrCreate()\n",
    "spark = SparkSession.builder.master('local[*]').appName('used-car-price')\\\n",
    ".config(\"spark.executor.instances\", \"1\")\\\n",
    ".config(\"spark.executor.memory\", \"6g\")\\\n",
    ".config(\"spark.driver.memory\", \"6g\")\\\n",
    ".config(\"spark.executor.memoryOverhead\", \"8g\")\\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = '../data/cleaned_test.csv'\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+--------------------+----------+------+---------+--------------------+--------------------+--------------------+-------+---------------+-----------+\n",
      "|    id|        brand|               model|model_year|milage|fuel_type|              engine|        transmission|             ext_col|int_col|       accident|clean_title|\n",
      "+------+-------------+--------------------+----------+------+---------+--------------------+--------------------+--------------------+-------+---------------+-----------+\n",
      "|218052|Mercedes-Benz|       E-Class E 350|      2014| 82900| Gasoline|302.0HP 3.5L V6 C...|   7-speed automatic|               White|  Beige|  None reported|        Yes|\n",
      "|218053|      Genesis|      G80 3.3T Sport|      2009|140401| Gasoline|365.0HP 3.3L V6 C...|   6-speed automatic|                Gray|  Black|  None reported|        Yes|\n",
      "|218054|         Ford|        F-150 Lariat|      2021|  3055| Gasoline|375.0HP 3.5L V6 C...|  10-speed automatic|               Green|  Black|  None reported|        Yes|\n",
      "|218055|Mercedes-Benz|AMG C 43 AMG C 43...|      2022|  3921| Gasoline|385.0HP 3.0L V6 C...|   9-speed automatic|               Black|   Gray|  None reported|        Yes|\n",
      "|218056|    Chevrolet|    Suburban Premier|      2018|  1025| Gasoline|355.0HP 5.3L 8 Cy...|   6-speed automatic|              Silver|  Black|damage reported|        Yes|\n",
      "|218058|         Ford|        Explorer XLT|      2017|161000| Gasoline|290.0HP 3.5L V6 C...|   6-speed automatic|               White|  Beige|damage reported|        Yes|\n",
      "|218059|      Hyundai|       Veloster Base|      2012|183000| Gasoline|132.0HP 1.6L 4 Cy...|transmission w/du...|               Black|   Gray|  None reported|        Yes|\n",
      "|218060|          BMW|             M3 Base|      2003| 86818| Gasoline|333.0HP 3.2L Stra...|      6-speed manual|               Black|  Black|damage reported|        Yes|\n",
      "|218061|    Chevrolet|Corvette Stingray...|      2020| 12500| Gasoline|490.0HP 6.2L 8 Cy...|transmission w/du...|                Gray|   Blue|  None reported|        Yes|\n",
      "|218062|         MINI|       Cooper S Base|      2008|132000| Gasoline|172.0HP 1.6L 4 Cy...|   6-speed automatic|               Black|  Black|damage reported|        Yes|\n",
      "|218063|      Porsche|             Macan S|      2017| 45578| Gasoline|340.0HP 3.0L V6 C...|   7-speed automatic|                 Red|    Red|damage reported|        Yes|\n",
      "|218064|         Audi|     A3 2.0T Premium|      2017| 77000| Gasoline|184.0HP 2.0L 4 Cy...|transmission w/du...|                 Red|  Black|  None reported|        Yes|\n",
      "|218065|        Lexus|RX 450h F Sport H...|      2022| 43671|   Hybrid|3.5L V6 24V PDI D...|continuously vari...|                Gray|  Black|  None reported|       NULL|\n",
      "|218066|         Land|Rover Range Rover...|      2015| 64517| Gasoline|518.0HP 5.0L 8 Cy...|transmission w/du...|               Black|  Brown|  None reported|        Yes|\n",
      "|218067|         Alfa|Romeo Stelvio Ti ...|      2019| 23344| Gasoline|2.0L I4 16V GDI S...|   8-speed automatic|Vulcano Black Met...|    Ice|  None reported|       NULL|\n",
      "|218068|Mercedes-Benz|SL-Class SL550 Ro...|      2014| 38600| Gasoline|382.0HP 5.5L 8 Cy...|   7-speed automatic|               White|  Brown|  None reported|        Yes|\n",
      "|218069|         Jeep|  Gladiator Overland|      2021| 18083| Gasoline|3.6L V6 24V MPFI ...|   8-speed automatic|Firecracker Red C...|  Black|  None reported|       NULL|\n",
      "|218071|         Land|   Rover Defender SE|      2023|   159| Gasoline|395.0HP 3.0L Stra...|   8-speed automatic|               Green|  Beige|  None reported|        Yes|\n",
      "|218072|          GMC|        Yukon XL SLT|      2016|138415| Gasoline|355.0HP 5.3L 8 Cy...|   6-speed automatic|               Black|  Beige|damage reported|        Yes|\n",
      "|218073|          RAM|      2500 Tradesman|      2010|170000| Gasoline|410.0HP 6.4L 8 Cy...|           automatic|                Blue|   Gray|  None reported|        Yes|\n",
      "+------+-------------+--------------------+----------+------+---------+--------------------+--------------------+--------------------+-------+---------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.drop('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "import pandas as pd\n",
    "def knn_impute(df, n_neighbors=5):   \n",
    "    df_encoded = df.copy()\n",
    "    for col in df_encoded.select_dtypes(include='object').columns:\n",
    "        df_encoded[col] = df_encoded[col].astype('category').cat.codes\n",
    "    knn_imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "    df_imputed = pd.DataFrame(knn_imputer.fit_transform(df_encoded), columns=df_encoded.columns)\n",
    "    for col in df.select_dtypes(include='object').columns:\n",
    "        df_imputed[col] = df_imputed[col].round().astype(int).map(\n",
    "            dict(enumerate(df[col].astype('category').cat.categories)))\n",
    "    return df_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_imputed = knn_impute(data.toPandas(), n_neighbors=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "cat_cols_train = df_train_imputed.select_dtypes(include=['object']).columns\n",
    "cat_cols_train = cat_cols_train[cat_cols_train != 'class']\n",
    "ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "\n",
    "df_train_imputed[cat_cols_train] = ordinal_encoder.fit_transform(df_train_imputed[cat_cols_train].astype(str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "brand           0\n",
       "model           0\n",
       "model_year      0\n",
       "milage          0\n",
       "fuel_type       0\n",
       "engine          0\n",
       "transmission    0\n",
       "ext_col         0\n",
       "int_col         0\n",
       "accident        0\n",
       "clean_title     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_imputed.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sp_data = spark.createDataFrame(df_train_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----------+--------+---------+------+------------+-------+-------+--------+-----------+\n",
      "|brand| model|model_year|  milage|fuel_type|engine|transmission|ext_col|int_col|accident|clean_title|\n",
      "+-----+------+----------+--------+---------+------+------------+-------+-------+--------+-----------+\n",
      "| 36.0| 549.0|    2014.0| 82900.0|      2.0| 583.0|        16.0|  302.0|   10.0|     0.0|        0.0|\n",
      "| 16.0| 754.0|    2009.0|140401.0|      2.0| 738.0|        11.0|  127.0|   14.0|     0.0|        0.0|\n",
      "| 14.0| 660.0|    2021.0|  3055.0|      2.0| 750.0|         1.0|  128.0|   14.0|     0.0|        0.0|\n",
      "| 36.0| 206.0|    2022.0|  3921.0|      2.0| 769.0|        22.0|   29.0|   71.0|     0.0|        0.0|\n",
      "|  9.0|1627.0|    2018.0|  1025.0|      2.0| 717.0|        11.0|  261.0|   14.0|     1.0|        0.0|\n",
      "| 14.0| 650.0|    2017.0|161000.0|      2.0| 478.0|        11.0|  302.0|   10.0|     1.0|        0.0|\n",
      "| 19.0|1743.0|    2012.0|183000.0|      2.0|  38.0|        30.0|   29.0|   71.0|     0.0|        0.0|\n",
      "|  4.0| 968.0|    2003.0| 86818.0|      2.0| 673.0|        14.0|   29.0|   14.0|     1.0|        0.0|\n",
      "|  9.0| 510.0|    2020.0| 12500.0|      2.0| 938.0|        30.0|  127.0|   28.0|     0.0|        0.0|\n",
      "| 31.0| 495.0|    2008.0|132000.0|      2.0| 116.0|        11.0|   29.0|   14.0|     1.0|        0.0|\n",
      "| 42.0|1028.0|    2017.0| 45578.0|      2.0| 685.0|        16.0|  232.0|  118.0|     1.0|        0.0|\n",
      "|  3.0| 167.0|    2017.0| 77000.0|      2.0| 156.0|        30.0|  232.0|   14.0|     0.0|        0.0|\n",
      "| 27.0|1328.0|    2022.0| 43671.0|      3.0| 539.0|        25.0|  127.0|   14.0|     0.0|        1.0|\n",
      "| 26.0|1399.0|    2015.0| 64517.0|      2.0| 977.0|        30.0|   29.0|   31.0|     0.0|        0.0|\n",
      "|  1.0|1370.0|    2019.0| 23344.0|      2.0| 202.0|        19.0|  301.0|   77.0|     0.0|        1.0|\n",
      "| 36.0|1493.0|    2014.0| 38600.0|      2.0| 767.0|        16.0|  302.0|   31.0|     0.0|        0.0|\n",
      "| 22.0| 816.0|    2021.0| 18083.0|      2.0| 546.0|        19.0|  102.0|   14.0|     0.0|        1.0|\n",
      "| 26.0|1375.0|    2023.0|   159.0|      2.0| 786.0|        19.0|  128.0|   10.0|     0.0|        0.0|\n",
      "| 15.0|1867.0|    2016.0|138415.0|      2.0| 717.0|        11.0|   29.0|   10.0|     1.0|        0.0|\n",
      "| 43.0|  30.0|    2010.0|170000.0|      2.0| 844.0|        24.0|   38.0|   71.0|     0.0|        0.0|\n",
      "+-----+------+----------+--------+---------+------+------------+-------+-------+--------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "Sp_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScalerModel\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Load the saved scaler model\n",
    "scaler_model = StandardScalerModel.load(\"scaler_model\")\n",
    "\n",
    "# Define feature columns for the test data\n",
    "feature_columns = [column for column in Sp_data.columns]  # Adjust if needed\n",
    "\n",
    "# Assemble features for test data\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "test_assembled_df = assembler.transform(Sp_data)\n",
    "\n",
    "# Use the pre-fitted scaler model to transform the test data\n",
    "scaled_test_df = scaler_model.transform(test_assembled_df)\n",
    "\n",
    "# Convert the 'scaled_features' vector to an array for easier handling\n",
    "scaled_test_df = scaled_test_df.withColumn(\"scaled_features_array\", vector_to_array(scaled_test_df[\"scaled_features\"]))\n",
    "\n",
    "# Get the number of features (same as training data)\n",
    "num_features = len(feature_columns)\n",
    "\n",
    "# Create individual columns for each feature in the array\n",
    "for i in range(num_features):\n",
    "    scaled_test_df = scaled_test_df.withColumn(f\"scaled_feature_{i+1}\", col(\"scaled_features_array\")[i])\n",
    "\n",
    "# Select only the individual scaled feature columns for saving\n",
    "scaled_test_to_save = scaled_test_df.select([f\"scaled_feature_{i+1}\" for i in range(num_features)])\n",
    "\n",
    "# Save the scaled test data to CSV\n",
    "scaled_test_to_save.write.csv(\"../data/scaled_test_data.csv\", header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNSUPPORTED_DATA_TYPE_FOR_DATASOURCE] The CSV datasource doesn't support the column `features` of the type \"STRUCT<type: TINYINT, size: INT, indices: ARRAY<INT>, values: ARRAY<DOUBLE>>\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/preprocessed_test.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mscaled_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/lang_chain/Storage/azure-code/.conda/lib/python3.10/site-packages/pyspark/sql/readwriter.py:1864\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m   1847\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[1;32m   1848\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1862\u001b[0m     lineSep\u001b[38;5;241m=\u001b[39mlineSep,\n\u001b[1;32m   1863\u001b[0m )\n\u001b[0;32m-> 1864\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/lang_chain/Storage/azure-code/.conda/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/media/lang_chain/Storage/azure-code/.conda/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNSUPPORTED_DATA_TYPE_FOR_DATASOURCE] The CSV datasource doesn't support the column `features` of the type \"STRUCT<type: TINYINT, size: INT, indices: ARRAY<INT>, values: ARRAY<DOUBLE>>\"."
     ]
    }
   ],
   "source": [
    "output_path = \"../data/preprocessed_test.csv\"\n",
    "scaled_df.write.csv(output_path, header=True, mode='overwrite')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
